// Copyright 2021 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package google.internal.federated.plan;

import "google/protobuf/any.proto";
import "tensorflow/core/example/feature.proto";
import "tensorflow/core/framework/tensor_shape.proto";
import "tensorflow/core/framework/types.proto";
import "tensorflow/core/protobuf/saver.proto";
import "tensorflow/core/protobuf/struct.proto";

option java_package = "com.google.internal.federated.plan";
option java_multiple_files = true;
option java_outer_classname = "PlanProto";

// Primitives
// ===========

// Represents an operation to save or restore from a checkpoint.  Some
// instances of this message may only be used either for restore or for
// save, others for both directions. This is documented together with
// their usage.
//
// This op has four essential uses:
//   1. read and apply a checkpoint.
//   2. write a checkpoint.
//   3. read and apply from an aggregated side channel.
//   4. write to a side channel (grouped with write a checkpoint).
// We should consider splitting this into four separate messages.
message CheckpointOp {
  // An optional standard saver def. If not provided, only the
  // op(s) below will be executed. This must be a version 1 SaverDef.
  tensorflow.SaverDef saver_def = 1;

  // An optional operation to run before the saver_def is executed for
  // restore.
  string before_restore_op = 2;

  // An optional operation to run after the saver_def has been
  // executed for restore. If side_channel_tensors are provided, then
  // they should be provided in a feed_dict to this op.
  string after_restore_op = 3;

  // An optional operation to run before the saver_def will be
  // executed for save.
  string before_save_op = 4;

  // An optional operation to run after the saver_def has been
  // executed for save. If there are side_channel_tensors, this op
  // should be run after the side_channel_tensors have been fetched.
  string after_save_op = 5;

  // In addition to being saved and restored from a checkpoint, one can
  // also save and restore via a side channel. The keys in this map are
  // the names of the tensors transmitted by the side channel. These (key)
  // tensors should be read off just before saving a SaveDef and used
  // by the code that handles the side channel. Any variables provided this
  // way should NOT be saved in the SaveDef.
  //
  // For restoring, the variables that are provided by the side channel
  // are restored differently than those for a checkpoint. For those from
  // the side channel, these should be restored by calling the before_restore_op
  // with a feed dict whose keys are the restore_names in the SideChannel and
  // whose values are the values to be restored.
  map<string, SideChannel> side_channel_tensors = 6;
}

message SideChannel {
  // A side channel whose variables are processed via SecureAggregation.
  // This side channel implements aggregation via sum over a set of
  // clients, so the restored tensor will be a sum of multiple clients
  // inputs into the side channel. Hence this will restore during the
  // read_aggregate_update restore, not the per-client read_update restore.
  message SecureAggregand {
    message Dimension {
      int64 size = 1;
    }

    // Dimensions of the aggregand. This is used by the secure aggregation
    // protocol in its early rounds, not as redundant info which could be
    // obtained by reading the dimensions of the tensor itself.
    repeated Dimension dimension = 3;

    // The data type anticipated by the server-side graph.
    tensorflow.DataType dtype = 4;

    // SecureAggregation will compute sum modulo this modulus.
    message FixedModulus {
      uint64 modulus = 1;
    }

    // SecureAggregation will for each shard compute sum modulo m with m at
    // least (1 + shard_size * (base_modulus - 1)), then aggregate
    // shard results with non-modular addition. Here, shard_size is the number
    // of clients in the shard.
    //
    // Note that the modulus for each shard will be greater than the largest
    // possible (non-modular) sum of the inputs to that shard.  That is,
    // assuming each client has input on range [0, base_modulus), the result
    // will be identical to non-modular addition (i.e. federated_sum).
    //
    // While any m >= (1 + shard_size * (base_modulus - 1)), the current
    // implementation takes
    // m = 2**ceil(log_2(1 + shard_size * (base_modulus - 1))), which is the
    // smallest possible value of m that is also a power of 2.  This choice is
    // made because (a) it uses the same number of bits per vector entry as
    // valid smaller m, using the current on-the-wire encoding scheme, and (b)
    // it enables the underlying mask-generation PRNG to run in its most
    // computationally efficient mode, which can be up to 2x faster.
    message ModulusTimesShardSize {
      uint64 base_modulus = 1;
    }

    oneof modulus_scheme {
      // Bitwidth of the aggregand.
      //
      // This is the bitwidth of an input value (i.e. the bitwidth that
      // quantization should target).  The Secure Aggregation bitwidth (i.e.,
      // the bitwidth of the *sum* of the input values) will be a function of
      // this bitwidth and the number of participating clients, as negotiated
      // with the server when the protocol is initiated.
      //
      // Deprecated; prefer fixed_modulus instead.
      int32 quantized_input_bitwidth = 2 [deprecated = true];

      FixedModulus fixed_modulus = 5;
      ModulusTimesShardSize modulus_times_shard_size = 6;
    }

    reserved 1;
  }

  // What type of side channel is used.
  oneof type {
    SecureAggregand secure_aggregand = 1;
  }

  // When restoring the name of the tensor to restore to.  This is the name
  // (key) supplied in the feed_dict in the before_restore_op in order to
  // restore the tensor provided by the side channel (which will be the
  // value in the feed_dict).
  string restore_name = 2;
}

// Container for a metric used by the internal toolkit.
message Metric {
  // Name of an Op to run to read the value.
  string variable_name = 1;

  // A human-readable name for the statistic. Metric names are usually
  // camel case by convention, e.g., 'Loss', 'AbsLoss', or 'Accuracy'.
  // Must be 7-bit ASCII and under 122 characters.
  string stat_name = 2;

  // The human-readable name of another metric by which this metric should be
  // normalized, if any. If empty, this Metric should be aggregated with simple
  // summation. If not empty, the Metric is aggregated according to
  // weighted_metric_sum = sum_i (metric_i * weight_i)
  // weight_sum = sum_i weight_i
  // average_metric_value = weighted_metric_sum / weight_sum
  string weight_name = 3;
}

// Controls the format of output metrics users receive. Represents instructions
// for how metrics are to be output to users, controlling the end format of
// the metric users receive.
message OutputMetric {
  // Metric name.
  string name = 1;

  oneof value_source {
    // A metric representing one stat with aggregation type sum.
    SumOptions sum = 2;

    // A metric representing a ratio between metrics with aggregation
    // type average.
    AverageOptions average = 3;

    // A metric that is not aggregated by the MetricReportAggregator or
    // metrics_loader. This includes metrics like 'num_server_updates' that are
    // aggregated in TensorFlow.
    NoneOptions none = 4;

    // A metric representing one stat with aggregation type only sample.
    // Samples at most 101 clients' values.
    OnlySampleOptions only_sample = 5;
  }
  // Iff True, the metric will be plotted in the default view of the
  // task level Colab automatically.
  oneof visualization_info {
    bool auto_plot = 6 [deprecated = true];
    VisualizationSpec plot_spec = 7;
  }
}

message VisualizationSpec {
  // Different allowable plot types.
  enum VisualizationType {
    NONE = 0;
    DEFAULT_PLOT_FOR_TASK_TYPE = 1;
    LINE_PLOT = 2;
    LINE_PLOT_WITH_PERCENTILES = 3;
    HISTOGRAM = 4;
  }

  // Defines the plot type to provide downstream.
  VisualizationType plot_type = 1;

  // The x-axis which to provide for the given metric. Must be the name of a
  // metric or counter. Recommended x_axis options are source_round, round,
  // or time.
  string x_axis = 2;

  // Iff True, metric will be displayed on a population level dashboard.
  bool plot_on_population_dashboard = 3;
}

// A metric representing one stat with aggregation type sum.
message SumOptions {
  // Name for corresponding Metric stat_name field.
  string stat_name = 1;

  // Iff True, a cumulative sum over rounds will be provided in addition to a
  // sum per round for the value metric.
  bool include_cumulative_sum = 2;

  // Iff True, sample of at most 101 clients' values.
  // Used to calculate quantiles in downstream visualization pipeline.
  bool include_client_samples = 3;
}

// A metric representing a ratio between metrics with aggregation type average.
// Represents: numerator stat / denominator stat.
message AverageOptions {
  // Numerator stat name pointing to corresponding Metric stat_name.
  string numerator_stat_name = 1;

  // Denominator stat name pointing to corresponding Metric stat_name.
  string denominator_stat_name = 2;

  // Name for corresponding Metric stat_name that is the ratio of the
  // numerator stat / denominator stat.
  string average_stat_name = 3;

  // Iff True, sample of at most 101 client's values.
  // Used to calculate quantiles in downstream visualization pipeline.
  bool include_client_samples = 4;
}

// A metric representing one stat with aggregation type none.
message NoneOptions {
  // Name for corresponding Metric stat_name field.
  string stat_name = 1;
}

// A metric representing one stat with aggregation type only sample.
message OnlySampleOptions {
  // Name for corresponding Metric stat_name field.
  string stat_name = 1;
}

// Represents a data set. This is used for testing.
message Dataset {
  // Represents the data set for one client.
  message ClientDataset {
    // A string identifying the client.
    string client_id = 1;

    // A list of serialized tf.Example protos.
    repeated bytes example = 2;

    // Represents a dataset whose examples are selected by an ExampleSelector.
    message SelectedExample {
      ExampleSelector selector = 1;
      repeated bytes example = 2;
    }

    // A list of (selector, dataset) pairs. Used in testing some *TFF-based
    // tasks* that require multiple datasets as client input, e.g., a TFF-based
    // personalization eval task requires each client to provide at least two
    // datasets: one for train, and the other for test.
    repeated SelectedExample selected_example = 3;
  }

  // A list of client data.
  repeated ClientDataset client_data = 1;
}

message HealthPredicates {
  reserved 1, 2;
  // The highest value peak memory is allowed to be. Note that this number
  // represents the max memory *growth* allowed caused by executing the task,
  // not the overall peak memory of the whole test process.
  int64 max_peak_memory_usage_bytes = 3;
}

// Represents predicates over metrics - i.e., expectations. This is used in
// training/eval tests to encode metric names and values expected to be reported
// by a client execution.
message MetricTestPredicates {
  // The value must lie in [lower_bound; upper_bound]. Can also be used for
  // approximate matching (lower == value - epsilon; upper = value + epsilon).
  message Interval {
    double lower_bound = 1;
    double upper_bound = 2;
  }

  // The value must be a real value as long as the value of the weight_name
  // metric is non-zero. If the weight metric is zero, then it is acceptable for
  // the value to be non-real.
  message RealIfNonzeroWeight {
    string weight_name = 1;
  }

  message MetricCriterion {
    // Name of the metric.
    string name = 1;

    // FL training round this metric is expected to appear in.
    int32 training_round_index = 2;

    // If none of the following is set, no matching is performed; but the
    // metric is still expected to be present (with whatever value).
    oneof Criterion {
      // The reported metric must be < lt.
      float lt = 3;
      // The reported metric must be > gt.
      float gt = 4;
      // The reported metric must be <= le.
      float le = 5;
      // The reported metric must be >= ge.
      float ge = 6;
      // The reported metric must be == eq.
      float eq = 7;
      // The reported metric must lie in the interval.
      Interval interval = 8;
      // The reported metric is not NaN or +/- infinity.
      bool real = 9;
      // The reported metric is real (i.e., not NaN or +/- infinity) if the
      // value of an associated weight is not 0.
      RealIfNonzeroWeight real_if_nonzero_weight = 10;
    }
  }

  repeated MetricCriterion metric_criterion = 1;

  HealthPredicates health_predicates = 2;
}

// A message for inference testing (input data + expected outputs).
// The following message contains inference input data + output expectations
// that are used in e.g. compatibility tests of the client side code, to
// ensure the code conforms to the test expectations.
message InferenceTestData {
  message InferenceTestPredicate {
    message FloatInterval {
      float lower = 1;
      float upper = 2;
    }
    message FloatIntervalList {
      repeated FloatInterval float_interval = 1;
    }

    // Name of the output.
    string name = 1;

    // A predicate that the inference output must fulfill.
    oneof Predicate {
      // An exact match for bytes_list.
      tensorflow.BytesList bytes_list = 2;
      // An exact match of an integer vector.
      tensorflow.Int64List int64_list = 3;
      // An interval match for float vectors (inclusive lower and upper bounds
      // for every element in the float vector). Setting lower == upper implies
      // an exact match.
      FloatIntervalList float_interval_list = 4;
    }
  }

  // An inference text example consists of input data (a tf.Example),
  // and expectations for the output (exact bytes or integer vector matches,
  // or interval matches for float vectors).
  message InferenceTestExample {
    // A serialized tf.Example proto.
    bytes input_data = 1;

    repeated InferenceTestPredicate output_expectations = 2;
  }

  repeated InferenceTestExample test_examples = 1;

  HealthPredicates health_predicates = 2;
}

// Client Phase
// ============

// A `TensorflowSpec` or list of `ClientExecution`s that are executed
// sequentially on the client in a single `tf.Session`. In federated
// optimization, this will correspond to one `ServerPhase`.
//
// It is an error for both `TensorflowSpec` and `ClientExecution` to be present.
message ClientPhase {
  // A short CamelCase name for the ClientPhase.
  string name = 2;

  // A list of ClientExecutions that should be performed sequentially
  // and constitute a phase.
  //
  // Only the first ClientExecution's read_client_init and the last
  // ClientExecution's write_update should be set. These correspond to
  // the checkpoints read from and sent to the server, respectively
  // stored in device memory.
  //
  // If populated, the `tensorflow_spec` field must be empty.
  repeated ClientExecution execution = 1;

  // Minimum number of clients in aggregation.
  // In secure aggregation mode this is used to configure the protocol instance
  // in a way that server can't learn aggregated values with number of
  // participants lower than this number.
  // Without secure aggregation server still respects this parameter,
  // ensuring that aggregated values never leave server RAM unless they include
  // data from (at least) specified number of participants.
  int32 minimum_number_of_participants = 3;

  // A functional interface for the TensorFlow logic the client should perform.
  //
  // If populated:
  //   - `execution` must be empty.
  //   - `io_router` must be specified.
  TensorflowSpec tensorflow_spec = 4 [lazy = true];

  // The specification of the inputs coming either from customer apps
  // (Local Compute) or the federated protocol (Federated Compute).
  oneof io_router {
    FederatedComputeIORouter federated_compute = 5 [lazy = true];
    LocalComputeIORouter local_compute = 6 [lazy = true];
    FederatedComputeEligibilityIORouter federated_compute_eligibility = 7
        [lazy = true];
  }
}

// A ClientExecution corresponds to a related set of Ops that make one
// or more passes (epochs) over the local training data. Multiple
// ClientExecutions run sequentially in the same phase, see
// ClientPhase.
//
// A typical execution loop is generally composed of the following
// steps:
//
// 1. Read checkpoint (if read_client_init is set).
// 2. Run before_op which does any local initialization and setup, and
//    in TFF-based plans that use `tf.data.Dataset`, drives all the
//    client-side work.
// 3. In non-TFF-based plans, loop across loop_op until training is complete.
//    Generally this will be defined by a certain number of training epochs.
// 4. Run after_op (if set).
// 5. Save checkpoint (if write_update is set).
//
// If new ops are added here, they should be added to plan_builder.py.
//
// NEXT TAG: 26
message ClientExecution {
  // A short CamelCase name for the ClientExecution.
  string name = 21;

  // An op to run before read_client_init.
  string init_op = 1;

  // Optional. If set, reads a checkpoint to initialize client state.
  // This is a restore-only CheckpointOp.
  CheckpointOp read_client_init = 2;

  // Optional. If set, is called after execution has finished to save
  // a checkpoint. Depending on the environment, the checkpoint is
  // published to the server or stored on the device. This is a
  // save-only CheckpointOp.
  CheckpointOp write_update = 3;

  // The stat_name of a metric which guards publishing the checkpoint
  // produced by a write_update. If this is not set, the checkpoint
  // will be unconditionally updated. If this is set, publishing will be
  // postponed until a subsequent execution produces a metric of this name
  // with a value greater than write_update_quality_threshold.
  // Only one checkpoint can exist which is pending publishing;
  // if an execution generates another write_update before the metric is
  // produced, an older checkpoint will be abandoned.
  string write_update_quality_metric = 15;

  // The threshold for the write update metric, a value between 0 and 1.
  // The metric must be greater than this value to let a checkpoint be
  // published. If its smaller, the checkpoint will be abandoned.
  double write_update_quality_threshold = 16;

  // Saves and restores all state required for this ClientExecution
  // and any subsequent ClientExecutions in the same ClientPhase, in
  // order to allow resuming computation after a session is interrupted
  // (e.g., shut down by Android).  The 'savepoint' is valid for
  // saving/restoring state both in between calls to the 'loop_op',
  // and after the ClientExecution completes.
  //
  // N.B. Even if a variable is not used by this ClientExecution, if
  // it is needed by a subsequent ClientExecution in the same
  // ClientPhase, it must still be saved by this saverdef.
  CheckpointOp savepoint = 4;

  // A specification of the example selection procedure.  This is
  // passed to the client application which delivers training examples
  // dataset.
  ExampleSelector example_selector = 5;

  // The number of passes over the dataset (epochs) to run the loop_op
  // specified below.
  int32 epochs_to_run = 6;

  // Feeds for providing input, or metadata about input, to the graph. See
  // comments below on when these are fed.
  oneof input_kind {
    // An input placeholder to fed examples to loop_op. This is an input
    // mechanism used by non-TFF plans in production.
    string input_feed = 7;

    // The name of a string placeholder containing the token to feed to the
    // custom dataset op fcp.tensorflow.external_dataset.ExternalDataset when
    // running a client graph that uses tf.data.Dataset (rather than a
    // push-based feed with a loop_op) to obtain data. The placeholder should be
    // fed a valid token string when running `before_op`.
    string external_dataset_token_feed = 24;

    AsyncInputFeed async_input_feed = 8 [deprecated = true];
    DatasetInput dataset_input = 22 [deprecated = true];
  }

  // The batch size for clustering examples into a single
  // Tensor. Graphs are expected to support arbitrary batch sizes, so
  // it is OK if some of the batches have different sizes (e.g. the
  // last one may be small). If not specified or 0, a value of 1 is assumed.
  //
  // This should only be set if batches are to be formed *outside* of
  // the TensorFlow graph, and fed in as complete batches, as in the
  // client code (see training/service2/TensorflowPhaseExecutor.java).
  // An alternative is to use queues to construct batches directly in
  // the TensorFlow graph, in which case this value should be set to
  // zero.
  int32 batch_size = 9;

  // Optional. Operation to run before loop_op.
  string before_op = 10;

  // Optional. The name of an operation to run many times. This is run
  // either in "feed" or in "queue" mode, depending on input_kind. In
  // feed mode, the op takes as parameter input_feed, which contains a
  // minibatch of example protos. The plan engine calls the loop as
  // often as needed to iterate through all examples in the dataset
  // returned by example_selector. In queue mode, the loop_op takes no
  // parameters, and instead input is read from input_queue; the loop
  // is run until the op returns status code OUT_OF_RANGE.
  string loop_op = 12;

  // Optional. The name of an operation to run once immediately after
  // all executions of the loop_op.
  string after_op = 13;

  // A list of scalar metrics which have been assigned to after
  // executing this ClientExecution. Used to generate client specific
  // metrics that are, say, logged via ClearCut. These will usually be
  // set in the evaluation ClientExecution of a client-only
  // computation to produce metric values that are locally consumed.
  repeated Metric stats = 14;

  reserved 11, 17, 18, 19, 20, 23, 25;
}

// TensorflowSpec message describes a single call into TensorFlow, including the
// expected input tensors that must be fed when making that call, which
// output tensors to be fetched, and any operations that have no output but must
// be run. The TensorFlow session will then use the input tensors to do some
// computation, generally reading from one or more datasets, and provide some
// outputs.
//
// Conceptually, the client code is going use this proto along with the IORouter
// to build maps of names to input tensors, vectors of output tensor names,
// and vectors of target nodes:
//
//    CreateTensorflowArugments(
//        TensorflowSpec& spec,
//        IORouter& io_router,
//        const vector<pair<string, Tensor>>* input_tensors,
//        const vector<string>* output_tensor_names,
//        const vector<string>* target_node_names);
//
// Where `input_tensor`, `output_tensor_names` and `target_node_names`
// correspond to the arguments of TensorFlow C++ API for
// `tensorflow::Session:Run()`, and the client executes only a single
// invocation.
//
// Note: the execution engine never sees any concepts related to the federated
// protocol, e.g. input checkpoints or aggregation protocols. This is a "tensors
// in, tensors out" interface. New aggregation methods can be added without
// having to modify the execution engine / TensorflowSpec message, instead they
// should modify the IORouter messages.
//
// Note: both `input_tensor_specs` and `output_tensor_specs` are full
// `tensorflow.TensorSpecProto` messages, though TensorFlow technically
// only requires the names to feed the values into the session. The additional
// dtypes/shape information must always be included in case the runtime
// executing this TensorflowSpec wants to perform additional, optional static
// assertions. The runtimes however are free to ignore the dtype/shapes and only
// rely on the names if so desired.
//
// Assertions:
//   - all names in `input_tensor_specs`, `output_tensor_specs`, and
//     `target_node_names` must appear in serialized GraphDef stored in
//     `Plan.client_graph_bytes`.
//   - `output_tensor_specs` or `target_node_names` must be non-empty, otherwise
//     there is nothing to execute in the graph.
message TensorflowSpec {
  // Tensor name into which the dataset token will be fed. The dataset token is
  // something that gets generated from within the execution engine.
  //
  // The dataset_token is a scalar string tensor and is separate from
  // `input_tensors` as there is only one.
  //
  // Note: a single dataset_token is valid for multiple `tf.data.Dataset`
  // objects. The token can be thought of as a handle to a dataset factory,
  // hence only one is needed.
  string dataset_token_tensor_name = 1;

  // TensorSpecs of inputs which will be passed to TF.
  //
  // Corresponds to the `feed_dict` parameter of `tf.Session.run()` in
  // TensorFlow's Python API, excluding the dataset_token listed above.
  //
  // Assertions:
  //   - All the tensor names designated as inputs in the
  //     FederatedComputeIORouter and LocalComputeIORouter must be listed
  //     here (otherwise the InputRouter work is unused).
  //   - All placeholders in the client graph must be listed here, with the
  //     exception of the dataset_token which is explicitly set above (otherwise
  //     TensorFlow will fail to execute).
  repeated tensorflow.TensorSpecProto input_tensor_specs = 2;

  // TensorSpecs that should be fetched from TF after execution.
  //
  // Corresponds to the `fetches` parameter of `tf.Session.run()` in
  // TensorFlow's Python API, and the `output_tensor_names` in TensorFlow's C++
  // API.
  //
  // Note: the SecAgg client will read the dimensions of the output tensor
  // specs for those that are configured in the `FederatedComputeIORouter` to go
  // back to the server using SecAgg.
  //
  // Assertions:
  //   - The set of tensor names here must strictly match all in the keys of
  //     `FederatedComputeIORouter.aggregations` (hence only includes tensors
  //     that will be communicated back to the server outside the TensorFlow
  //     checkpoint).
  //   - Local Compute tasks must not set this field.
  repeated tensorflow.TensorSpecProto output_tensor_specs = 3;

  // Node names in the graph that should be executed, but the output not
  // returned.
  //
  // Corresponds to the `fetches` parameter of `tf.Session.run()` in
  // TensorFlow's Python API, and the `target_node_names` in TensorFlow's C++
  // API.
  //
  // This is intended for use with operations that do not produce tensors, but
  // nonetheless are required to run (e.g. serializing checkpoints).
  repeated string target_node_names = 4;
}

// The input and output router for Federated Compute plans.
//
// This proto is the glue between the federated protocol and the TensorFlow
// execution engine. This message describes how to prepare data coming from the
// incoming `CheckinResponse` (defined in
// fcp/protos/federated_api.proto) for the `TensorflowSpec`, and what
// to do with outputs from `TensorflowSpec` (e.g. how to aggregate them back on
// the server).
//
// TODO(team) we could replace `input_checkpoint_file_tensor_name` with
// an `input_tensors` field, which would then be a tensor that contains the
// input TensorProtos directly and skipping disk I/O, rather than referring to a
// checkpoint file path.
message FederatedComputeIORouter {
  // ===========================================================================
  //   Inputs
  // ===========================================================================
  // The name of the scalar string tensor that is fed the file path to the
  // initial checkpoint (e.g. as provided via AcceptanceInfo.init_checkpoint).
  //
  // The federated protocol code would copy the `CheckinResponse`'s initial
  // checkpoint to a temporary file and then pass that file path through this
  // tensor.
  //
  // Ops may be added to the client graph that take this tensor as input and
  // reads the path.
  //
  // This field is optional. It may be omitted if the client graph does not use
  // an initial checkpoint.
  string input_filepath_tensor_name = 1;

  // The name of the scalar string tensor that is fed the file path to which
  // client work should serialize the bytes to send back to the server.
  //
  // The federated protocol code generates a temporary file and passes the file
  // path through this tensor.
  //
  // Ops may be be added to the client graph that use this tensor as an argument
  // to write files (e.g. writing checkpoints to disk).
  //
  // This field is optional. It must be omitted if the client graph does not
  // generate any output files (e.g. when all output tensors of `TensorflowSpec`
  // use Secure Aggregation). If this field is not set, then the `ReportRequest`
  // message in the federated protocol will not have the
  // `Report.update_checkpoint` field set. This absense of a value here can be
  // used to validate that the plan only uses Secure Aggregation.
  //
  // Conversely, if this field is set and executing the associated
  // TensorflowSpec does not write to the path is indication of an internal
  // framework error. The runtime should notify the caller that the computation
  // was setup incorrectly.
  string output_filepath_tensor_name = 2;

  // ===========================================================================
  //   Outputs
  // ===========================================================================
  // Describes which output tensors should be aggregated using an aggregation
  // protocol, and the configuration for those protocols.
  //
  // Assertions:
  //   - All keys must exist in the associated `TensorflowSpec` as
  //     `output_tensor_specs.name` values.
  map<string, AggregationConfig> aggregations = 3;
}

// The specification for how to aggregate the associated tensor across clients
// on the server.
message AggregationConfig {
  oneof protocol_config {
    // Indicates that the given output tensor should be processed using Secure
    // Aggregation, using the specified config options.
    SecureAggregationConfig secure_aggregation = 2;

    // Note: in the future we could add a `SimpleAggregationConfig` to add
    // support for simple aggregation without writing to an intermediate
    // checkpoint file first.
  }
}

// Parameters for the SecAgg protocol (go/secagg).
//
// Currently only the server uses the SecAgg parameters, so we only use this
// message to signify usage of SecAgg.
message SecureAggregationConfig {}

// The input and output router for eligibility-computing plans. These plans
// compute which other plans a client is eligible to run, and are returned by
// clients via a `EligibilityEvalCheckinResponse` (defined in
// fcp/protos/federated_api.proto).
message FederatedComputeEligibilityIORouter {
  // The name of the scalar string tensor that is fed the file path to the
  // initial checkpoint (e.g. as provided via
  // `EligibilityEvalPayload.init_checkpoint`).
  //
  // For more detail see the
  // `FederatedComputeIoRouter.input_filepath_tensor_name`, which has the same
  // semantics.
  //
  // This field is optional. It may be omitted if the client graph does not use
  // an initial checkpoint.
  //
  // This tensor name must exist in the associated
  // `TensorflowSpec.input_tensor_specs` list.
  string input_filepath_tensor_name = 1;

  // Name of the output tensor (a string scalar) containing the serialized
  // `google.internal.federatedml.v2.TaskEligibilityInfo` proto output. The
  // client code will parse this proto and place it in the
  // `task_eligibility_info` field of the subsequent `CheckinRequest`.
  //
  // This tensor name must exist in the associated
  // `TensorflowSpec.output_tensor_specs` list.
  string task_eligibility_info_tensor_name = 2;
}

// The input and output router for Local Compute plans.
//
// This proto is the glue between the customers app and the TensorFlow
// execution engine. This message describes how to prepare data coming from the
// customer app (e.g. the input directory the app setup), and the temporary,
// scratch output directory that will be notified to the customer app upon
// completion of `TensorflowSpec`.
message LocalComputeIORouter {
  // ===========================================================================
  //   Inputs
  // ===========================================================================
  // The name of a scalar string tensor that will contain the input directory
  // path.
  //
  // Apps will have the ability to create contracts between their Android code
  // and Local Compute toolkit code to place files in this directory with known
  // names (Android code) and create graphs with ops to read from the directory
  // + known name (toolkit code).
  string input_dir_tensor_name = 1;

  // Scalar string tensor name that will contain the output directory path.
  //
  // The provided directory should be considered temporary scratch that will be
  // deleted, not persisted. It is the responsibility of the calling app to
  // move the desired files to a permanent location once the client returns this
  // directory back to the calling app.
  string output_dir_tensor_name = 2;

  // ===========================================================================
  //   Outputs
  // ===========================================================================
  // NOTE: LocalCompute has no outputs other than what the client graph writes
  // to `output_dir` specified above.
}

// Describes a queue to which input is fed.
message AsyncInputFeed {
  // The op for enqueuing an example input.
  string enqueue_op = 1;

  // The input placeholders for the enqueue op.
  repeated string enqueue_params = 2;

  // The op for closing the input queue.
  string close_op = 3;

  // Whether the work that should be fed asynchronously is the data itself
  // or a description of where that data lives.
  bool feed_values_are_data = 4;
}

message DatasetInput {
  // Initializer of iterator corresponding to tf.data.Dataset object which
  // handles the input data. Stores name of an op in the graph.
  string initializer = 1;

  // Placeholders necessary to initialize the dataset.
  DatasetInputPlaceholders placeholders = 2;

  // Batch size to be used in tf.data.Dataset.
  int32 batch_size = 3;
}

message DatasetInputPlaceholders {
  // Name of placeholder corresponding to filename(s) of SSTable(s) to read data
  // from.
  string filename = 1;

  // Name of placeholder corresponding to key_prefix initializing the
  // SSTableDataset. Note the value fed should be unique user id, not a prefix.
  string key_prefix = 2;

  // Name of placeholder corresponding to number of rounds the local training
  // should be run for.
  string num_epochs = 3;

  // Name of placeholder corresponding to batch size.
  string batch_size = 4;
}

// Specifies an example selection procedure.
message ExampleSelector {
  // Selection criteria following a contract agreed upon between client and
  // model designers.
  google.protobuf.Any criteria = 1;

  // A URI identifying the example collection to read from. Format should adhere
  // to "${COLLECTION}://${APP_NAME}${COLLECTION_NAME}". The URI segments
  // should adhere to the following rules:
  // - The scheme ${COLLECTION} should be one of:
  //   - "app" for app-hosted example
  //   - "simulation" for collections not connected to an app (e.g., if used
  //     purely for simulation)
  // - The authority ${APP_NAME} identifies the owner of the example
  //   collection and should be either the app's package name, or be left empty
  //   (which means "the current app package name").
  // - The path ${COLLECTION_NAME} can be any valid URI path. NB It starts with
  //   a forward slash ("/").
  // - The query and fragment are currently not used, but they may become used
  //   for something in the future. To keep open that possibility they must
  //   currently be left empty.
  //
  // Example: "app://com.google.some.app/someCollection/name"
  // identifies the collection "/someCollection/name" owned and hosted by the
  // app with package name "com.google.some.app".
  //
  // Example: "app:/someCollection/name" or "app:///someCollection/name"
  // both identify the collection "/someCollection/name" owned and hosted by the
  // app associated with the training job in which this URI appears.
  //
  // The path will not be interpreted by the runtime, and will be passed to the
  // example collection implementation for interpretation. Thus, in the case of
  // app-hosted example stores, the path segment's interpretation is a contract
  // between the app's example store developers, and the app's model designers.
  //
  // If an `app://` URI is set, then the `TrainerOptions` collection name must
  // not be set.
  string collection_uri = 2;

  // Resumption token following a contract agreed upon between client and
  // model designers.
  google.protobuf.Any resumption_token = 3;
}

// Server Phase
// ============

// Represents a server phase which implements aggregation of multiple
// client updates.
//
// There are two different modes of aggregation that are described
// by the values in this message. The first is aggregation that is
// coming from coordinated sets of clients. This includes aggregation
// done via checkpoints from clients or aggregation done over a set
// of clients by a process like secure aggregation. The results of
// this first aggregation are saved to intermediate aggregation
// checkpoints. The second aggregation then comes from taking
// these intermediate checkpoints and aggregating over them.
//
// These two different modes of aggregation are done on different
// servers, the first in the 'L1' servers and the second in the
// 'L2' servers, so we use this nomenclature to describe these
// phases below.
//
message ServerPhase {
  // A short CamelCase name for the ServerPhase.
  string name = 8;

  // ===========================================================================
  // L1 "Intermediate" Aggregation.
  //
  // This is the initial aggregation that creates partial aggregates from client
  // results. L1 Aggregation may be run on many different instances.
  //
  // Pre-condition:
  //   The execution environment has loaded the graph from `server_graph_bytes`.

  // 1. Initialize the phase.
  //
  // Operation to run before the first aggregation happens.
  // For instance, clears the accumulators so that a new aggregation can begin.
  string phase_init_op = 1;

  // 2. For each client in set of clients:
  //   a. Restore variables from the client checkpoint.
  //
  // Loads a checkpoint from a single client written via
  // `ClientExecution.write_update`.  This is done once for every client
  // checkpoint in a round.
  CheckpointOp read_update = 3;
  //   b. Aggregate the data coming from the client checkpoint.
  //
  // An operation that aggregates the data from read_update.
  // Generally this will add to accumulators and it may leverage internal data
  // inside the graph to adjust the weights of the Tensors.
  //
  // Executed once for each `read_update`, to (for example) update accumulator
  // variables using the values loaded during `read_update`.
  string aggregate_into_accumulators_op = 4;

  // 3. After all clients have been aggregated, possibly restore
  //    variables that have been aggregated via a separate process.
  //
  // Optionally restores variables where aggregation is done across
  // an entire round of client data updates. In contrast to `read_update`,
  // which restores once per client, this occurs after all clients
  // in a round have been processed. This allows, for example, side
  // channels where aggregation is done by a separate process (such
  // as in secure aggregation), in which the side channel aggregated
  // tensor is passed to the `before_restore_op` which ensure the
  // variables are restored properly. The `after_restore_op` will then
  // be responsible for performing the accumulation.
  //
  // Note that in current use this should not have a SaverDef, but
  // should only be used for side channels.
  CheckpointOp read_aggregated_update = 10;

  // 4. Write the aggregated variables to an intermediate checkpoint.
  //
  // We require that `aggregate_into_accumulators_op` is associative and
  // commutative, so that the aggregates can be computed across
  // multiple TensorFlow sessions.
  // As an example, say we are computing the sum of 5 client updates:
  //    A = X1 + X2 + X3 + X4 + X5
  // We can always do this in one session by calling `read_update`j and
  // `aggregate_into_accumulators_op` once for each client checkpoint.
  //
  // Alternatively, we could compute:
  //    A1 = X1 + X2 in one TensorFlow session, and
  //    A2 = X3 + X4 + X5 in a different session.
  // Each of these sessions can then write their accumulator state
  // with the `write_intermediate_update` CheckpointOp, and a yet another third
  // session can then call `read_intermediate_update` and
  // `aggregate_into_accumulators_op` on each of these checkpoints to compute:
  //    A = A1 + A2 = (X1 + X2) + (X3 + X4 + X5).
  CheckpointOp write_intermediate_update = 7;
  // End L1 "Intermediate" Aggregation.
  // ===========================================================================

  // ===========================================================================
  // L2 Aggregation and Coordinator.
  //
  // This aggregates intermediate checkpoints from L1 Aggregation and performs
  // the finalizing of the update. Unlike L1 there will only be one instance
  // that does this aggregation.

  // Pre-condition:
  //   The execution environment has loaded the graph from `server_graph_bytes`
  //   and restored the global model using `server_savepoint` from the parent
  //   `Plan` message.

  // 1. Initialize the phase.
  //
  // This currently re-uses the `phase_init_op` from L1 aggregation above.

  // 2. Write a checkpoint that can be sent to the client.
  //
  // Generates a checkpoint to be sent to the client, to be read by
  // `ClientExecution.read_client_init`.

  CheckpointOp write_client_init = 2;

  // 3. For each intermediate checkpoint:
  //   a. Restore variables from the intermediate checkpoint.
  //
  // The corresponding read checkpoint op to the write_intermediate_update.
  // This is used instead of read_update for intermediate checkpoints because
  // the format of these updates may be different than those used in updates
  // from clients (which may, for example, be compressed).
  CheckpointOp read_intermediate_update = 9;
  //   b. Aggregate the data coming from the intermediate checkpoint.
  //
  // An operation that aggregates the data from `read_intermediate_update`.
  // Generally this will add to accumulators and it may leverage internal data
  // inside the graph to adjust the weights of the Tensors.
  string intermediate_aggregate_into_accumulators_op = 11;

  // 4. Write the aggregated intermediate variables to a checkpoint.
  //
  // This is used for downstream, cross-round aggregation of metrics.
  // These variables will be read back into a session with
  // read_intermediate_update.
  CheckpointOp write_accumulators = 12;

  // 5. Finalize the round.
  //
  // This can include:
  // - Applying the update aggregated from the intermediate checkpoints to the
  //   global model and other updates to cross-round state variables.
  // - Computing final round metric values (e.g. the `report` of a
  //   `tff.federated_aggregate`).
  string apply_aggregrated_updates_op = 5;

  // 5. Fetch the server aggregated metrics.
  //
  // A list of names of metric variables to fetch from the TensorFlow session.
  repeated Metric metrics = 6;

  // 6. Serialize the updated server state (e.g. the coefficients of the global
  //    model in FL) using `server_savepoint` in the parent `Plan` message.

  // End L2 Aggregation.
  // ===========================================================================
}

// Represents the server phase in an eligibility computation.
//
// This phase produces a checkpoint to be sent to clients. This checkpoint is
// then used as an input to the clients' task eligibility computations.
// This phase *does not include any aggregation.*
message ServerEligibilityComputationPhase {
  // A short CamelCase name for the ServerEligibilityComputationPhase.
  string name = 1;

  // The names of the TensorFlow nodes to run in order to produce output.
  repeated string target_node_names = 2;

  // The specification of inputs and outputs to the TensorFlow graph.
  oneof server_eligibility_io_router {
    TEContextServerEligibilityIORouter task_eligibility = 3 [lazy = true];
  }
}

// Represents the inputs and outputs of a `ServerEligibilityComputationPhase`
// which takes a single `TaskEligibilityContext` as input.
message TEContextServerEligibilityIORouter {
  // The name of the scalar string tensor that must be fed a serialized
  // `TaskEligibilityContext`.
  string context_proto_input_tensor_name = 1;

  // The name of the scalar string tensor that must be fed the path to which
  // the server graph should write the checkpoint file to be sent to the client.
  string output_filepath_tensor_name = 2;
}

// Plan
// =====

// Represents the overall plan for performing federated optimization
// or personalization, as handed over to the production system.
// This will typically be split down into individual pieces for
// different production parts, e.g. server and client side.
// NEXT_TAG: 13
message Plan {
  reserved 1, 3;

  // Optional. The TensorFlow graph used for all server processing. For
  // personalization, this will not be set.
  //
  // The actual type is expected to be tensorflow.GraphDef.
  // The TensorFlow graph is stored in serialized form for two reasons.
  // 1) We may use execution engines other than TensorFlow.
  // 2) We wish to avoid the cost of deserialized and re-serializing large
  // graphs, in the Federated Learning service.
  google.protobuf.Any server_graph_bytes = 7;

  // A savepoint to sync the server checkpoint with a persistent
  // storage system.  The storage initially holds a seeded checkpoint
  // which can subsequently read and updated by this savepoint.
  // Optional-- not present in eligibility computation plans (those with a
  // ServerEligibilityComputationPhase).
  CheckpointOp server_savepoint = 2;

  // Required. The TensorFlow graph used for all client executions.
  // The actual type is expected to be tensorflow.GraphDef.
  // The TensorFlow graph is stored in serialized form for two reasons.
  // 1) We may use execution engines other than TensorFlow.
  // 2) We wish to avoid the cost of deserialized and re-serializing large
  // graphs, in the Federated Learning service.
  google.protobuf.Any client_graph_bytes = 8;

  // Optional. The FlatBuffer used for TFLite training.
  // It contains the same model information as the client_graph_bytes, but with
  // a different format.
  bytes client_tflite_graph_bytes = 12;

  // A pair of client phase and server phase which are processed in
  // sync. The server execution defines how the results of a client
  // phase are aggregated, and how the checkpoints for clients are
  // generated.
  message Phase {
    // Required. The client phase.
    ClientPhase client_phase = 1;

    // Optional. Server phase; not provided for personalization or
    // eligibility tasks.
    ServerPhase server_phase = 2;

    // Optional. Only provided for eligibility tasks.
    ServerEligibilityComputationPhase server_eligibility_phase = 3;
  }

  // A pair of client and server computations to run.
  repeated Phase phase = 4;

  // Optional phase for calculating accuracy or loss. Not used on
  // production side currently.
  // Deprecated: See b/64019527.
  Phase evaluation = 5 [deprecated = true];

  // Metrics that are persistent across different phases. This
  // includes, for example, counters that track how much work of
  // different kinds has been done.
  repeated Metric metrics = 6;

  // Describes how metrics in both the client and server phases should be
  // aggregated.
  repeated OutputMetric output_metrics = 10;

  // Version of the plan:
  // version == 0 - Old plan without version field, containing b/65131070
  // version >= 1 - plan supports multi-shard aggregation mode (L1/L2)
  int32 version = 9;

  // A TensorFlow ConfigProto packed in an Any.
  //
  // If this field is unset, if the Any proto is set but empty, or if the Any
  // proto is populated with an empty ConfigProto (i.e. its `type_url` field is
  // set, but the `value` field is empty) then the client implementation may
  // choose a set of configuration parameters to provide to TensorFlow by
  // default.
  //
  // In all other cases this field must contain a valid packed ConfigProto
  // (invalid values will result in an error at execution time), and in this
  // case the client will not provide any other configuration parameters by
  // default.
  google.protobuf.Any tensorflow_config_proto = 11;
}

// Represents a client part of the plan of federated optimization.
// This also used to describe a client-only plan for standalone on-device
// training, known as personalization.
// NEXT_TAG: 6
message ClientOnlyPlan {
  // The graph to use for training, in binary form.
  bytes graph = 1;

  // Optional. The flatbuffer used for TFLite training.
  // Whether "graph" or "tflite_graph" is used for training is up to the client
  // code to allow for a flag-controlled a/b rollout.
  bytes tflite_graph = 5;

  // The client phase to execute.
  ClientPhase phase = 2;

  // Describes how metrics should be aggregated. Deprecated and unused.
  // TODO(team): Remove field after toolkit cleanup.
  repeated OutputMetric output_metrics = 3 [deprecated = true];

  // A TensorFlow ConfigProto.
  google.protobuf.Any tensorflow_config_proto = 4;
}

// Represents the cross round aggregation portion for user defined measurements.
// This is used by tools that process / analyze accumulator checkpoints
// after a round of computation, to achieve aggregation beyond a round.
message CrossRoundAggregationExecution {
  // Operation to run before reading accumulator checkpoint.
  string init_op = 1;

  // Reads accumulator checkpoint.
  CheckpointOp read_aggregated_update = 2;

  // Operation to merge loaded checkpoint into accumulator.
  string merge_op = 3;

  // Reads and writes the final aggregated accumulator vars.
  CheckpointOp read_write_final_accumulators = 6;

  // Metadata for mapping the TensorFlow `name` attribute of the `tf.Variable`
  // to the user defined name of the signal.
  repeated Measurement measurements = 4;

  // The `tf.Graph` used for aggregating accumulator checkpoints when
  // loading metrics.
  google.protobuf.Any cross_round_aggregation_graph_bytes = 5;
}

message Measurement {
  // Name of a TensorFlow op to run to read/fetch the value of this measurement.
  string read_op_name = 1;

  // A human-readable name for the measurement. Names are usually
  // camel case by convention, e.g., 'Loss', 'AbsLoss', or 'Accuracy'.
  string name = 2;

  reserved 3;

  // A serialized `tff.Type` for the measurement.
  bytes tff_type = 4;
}

// Inference Plan
// ==============

// Inference Plan is now DEPRECATED. Please use Local Computation instead.
message InferencePlan {
  option deprecated = true;

  // Inference graph, built using the inference mode on the ModelStamper.
  bytes inference_graph = 1;

  // A CheckpointOp specifying how to restore a checkpoint from disk.
  // Note: the fields related to saving should be left unset.
  CheckpointOp restore_op = 2;

  message InputDescription {
    // The human-assigned name of the input feed.
    string common_name = 1;
    // The (possibly generated) name used to feed this input tensor.
    string tensor_name = 2;

    // Shape of the input the graph is expecting to receive.
    // Note that shape[0] may be 0, indicating arbitrary batch sizes.
    tensorflow.TensorShapeProto shape = 3;
    // The data type the graph expects this feed to be.
    tensorflow.DataType dtype = 4;
  }

  // Native TensorFlow code accepts a list of Tensor names for producing session
  // output.  Unfortunately, the Tensor names are difficult to specify in a user
  // friendly way.  So we maintain a list of string pairs:
  // The common name, specified by the user, can be used to access the
  // Tensor name, which can be used to access the Tensor itself.
  message KeyPair {
    string common_name = 1;
    string tensor_name = 2;
  }

  // Was 'ranking' when multiple inference_plan types were allowed.
  reserved 3;
  // Was 'input_format' when inference also supported SequenceExample.
  reserved 4;

  // The name of the input feeds.
  // The Tensors supplied with this name should match the format expected by
  // the InputDecoderStamper.
  repeated InputDescription input_feeds = 5;

  // The inference plan output nodes.  This field basically holds the dictionary
  // provided as the 'inference_outputs' parameter of the ModelOutput object
  // returned by the ModelExecutionStamper.  The 'common_name' is the key string
  // provided by the stamper author.  The 'tensor_name' is the actual node name.
  // The pairing exists because the developer doesn't always have control over a
  // node name.  Allowing the developer to provide a 'common_name' makes it
  // possible for the developer to refer to a node 'by name' using the name
  // defined by the developer.
  repeated KeyPair output_keys = 6;

  // A TensorFlow ConfigProto.
  google.protobuf.Any tensorflow_config_proto = 7;
}
