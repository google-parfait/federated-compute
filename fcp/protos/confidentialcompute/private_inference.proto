// Copyright 2023 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package fcp.confidentialcompute;

// A customer-facing interface for configuring private inference queries in a
// TEE-hosted federated analytics workload.
message InferenceConfiguration {
  // A set of private inference tasks to execute inside the TEE-hosted
  // container using client data.
  repeated InferenceTask inference_task = 1;
  // The model and its configurations to be used for private inference.
  ModelConfiguration model_config = 2;
}

// A single private inference task to execute inside the TEE-hosted
// container. For Gemma, each task involves prompting the Gemma model once using
// input from one column and outputting the responses into another column.
message InferenceTask {
  // Data columns used for inference and for storing inference results.
  ColumnConfiguration column_config = 1;
  // The logic used for inference.
  InferenceLogic inference_logic = 2;
}

// Configuration for columns used for inference. Currently, only one input
// column is allowed to be used as inference input, and inference result will be
// saved in one output column.
message ColumnConfiguration {
  string input_column_name = 1;
  string output_column_name = 2;
}

// Logics used for different models to perform inference.
message InferenceLogic {
  oneof kind {
    Prompt prompt = 1;
  }
}

// A message for storing logics required for prompting a model.
message Prompt {
  string prompt_template = 1;
}

// Configuration of different models to be used for inference.
message ModelConfiguration {
  oneof kind {
    GemmaConfiguration gemma_config = 1;
  }
}

// Configuration for running inference using Gemma models.
message GemmaConfiguration {
  // The path to the model tokenizer file. Tokenizer file does not require
  // chunking because it is less than 2GB in size.
  string tokenizer_file = 1;
  // The model weight file split into multiple chunks, each under 2GB in size.
  repeated string model_weight_chunk_file = 2;
}
